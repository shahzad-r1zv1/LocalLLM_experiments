{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyN1pmVnRsxfXZup6fpp4GNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzad-r1zv1/LocalLLM_experiments/blob/main/GithubTalkie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================================\n",
        "# Federated Multi-Repo Chat  — Enhanced, Language-Independent RAG\n",
        "# --------------------------------------------\n",
        "# What this notebook does:\n",
        "# 1) Clone a curated list of GitHub repos into Colab\n",
        "# 2) Collect relevant code/docs/config files (language independent)\n",
        "# 3) Build per-repo maps (tree + entrypoint hints) for fast orientation\n",
        "# 4) Chunk files (Tree-sitter when possible, fallback chunking otherwise)\n",
        "# 5) Build a single *federated* hybrid search index across all repos:\n",
        "# - BM25 (lexical) helps with exact identifiers/symbols\n",
        "# - Embeddings + FAISS (semantic) helps with meaning & paraphrases\n",
        "# 6) Provide tools for repo exploration (open_file, list_tree, search_symbol, find_references)\n",
        "# 7) Ask questions either scoped to one repo or across all repos (repo='all')\n",
        "# 8) Apply simple confidence guardrails to reduce hallucinations\n",
        "#\n",
        "#\n",
        "#\n",
        "# ============================================\n",
        "# Why this approach works:\n",
        "# - Hybrid retrieval reduces the classic RAG failure mode where embeddings miss exact names.\n",
        "# - Tree-sitter chunking tends to keep functions/classes intact, improving answer quality.\n",
        "# - Federated indexing lets you find patterns across repos and compare implementations.\n",
        "# ============================================"
      ],
      "metadata": {
        "id": "v6NubQ7Je7CJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UP38X8uaNVj"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# CELL 0 — Install dependencies\n",
        "# ----------------------------------------------------------\n",
        "# gitpython: clone repos\n",
        "# tree_sitter + tree_sitter_languages: syntax-aware chunking\n",
        "# rank-bm25: lexical retrieval (identifiers, file names, exact tokens)\n",
        "# sentence-transformers: embeddings\n",
        "# faiss: fast vector search\n",
        "# transformers/bitsandbytes: load an open-source LLM in Colab (4-bit)\n",
        "# ==========================================================\n",
        "!pip -q install gitpython faiss-cpu sentence-transformers rank-bm25 transformers accelerate bitsandbytes\n",
        "!pip -q install tree_sitter tree_sitter_languages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELL 1 — Configure curated repo list + clone\n",
        "# ----------------------------------------------------------\n",
        "# You control the list. Each repo gets its own folder under ROOT_DIR.\n",
        "# Using a curated list keeps noise down and makes cross-repo linking meaningful.\n",
        "# ==========================================================\n",
        "import os, re, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from git import Repo\n",
        "\n",
        "REPOS = [\n",
        "    {\"name\": \"repoA\", \"url\": \"https://github.com/droidrun/droidrun\"},\n",
        "    {\"name\": \"repoB\", \"url\": \"https://github.com/mvysny/karibu-testing\"},\n",
        "    # {\"name\": \"repoC\", \"url\": \"https://github.com/OWNER_C/REPO_C\"},\n",
        "]\n",
        "\n",
        "ROOT_DIR = \"/content/repos\"\n",
        "\n",
        "if os.path.exists(ROOT_DIR):\n",
        "    shutil.rmtree(ROOT_DIR)\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "repo_dirs = {}  # repo_name -> local path\n",
        "\n",
        "for r in REPOS:\n",
        "    name, url = r[\"name\"], r[\"url\"]\n",
        "    dest = os.path.join(ROOT_DIR, name)\n",
        "    Repo.clone_from(url, dest)\n",
        "    repo_dirs[name] = dest\n",
        "    print(f\"Cloned {name}: {url}\")\n",
        "\n",
        "print(\"\\nRepos ready:\", list(repo_dirs.keys()))\n"
      ],
      "metadata": {
        "id": "KhmP8Riwassw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================================\n",
        "# CELL 2 — Collect files (language independent)\n",
        "# ----------------------------------------------------------\n",
        "# Goal: include source + docs + common config files; exclude build/vendor/cache.\n",
        "# This keeps indexing cost down and improves retrieval quality.\n",
        "# ==========================================================\n",
        "INCLUDE_EXT = {\n",
        "    \".py\", \".js\", \".ts\", \".jsx\", \".tsx\", \".java\", \".go\", \".rs\",\n",
        "    \".cpp\", \".c\", \".h\", \".hpp\", \".cs\", \".php\", \".rb\", \".swift\", \".kt\", \".scala\",\n",
        "    \".sql\", \".md\", \".rst\", \".txt\", \".yaml\", \".yml\", \".toml\", \".ini\", \".json\", \".xml\",\n",
        "    \".gradle\", \".properties\", \".cfg\", \".env\", \".sh\", \".ps1\"\n",
        "}\n",
        "\n",
        "EXCLUDE_DIRS = {\n",
        "    \".git\", \"node_modules\", \"dist\", \"build\", \"target\", \".venv\", \"venv\", \"__pycache__\",\n",
        "    \".next\", \".cache\", \".idea\", \".vscode\", \"coverage\", \".pytest_cache\", \".mypy_cache\",\n",
        "    \".gradle\", \".terraform\", \".npm\", \".yarn\", \".pnpm-store\", \".cargo\", \".tox\"\n",
        "}\n",
        "\n",
        "MAX_FILE_BYTES = 1_200_000  # skip files > 1.2MB (tune for your repos)\n",
        "\n",
        "\n",
        "def is_excluded(path: str) -> bool:\n",
        "    parts = set(Path(path).parts)\n",
        "    return len(parts.intersection(EXCLUDE_DIRS)) > 0\n",
        "\n",
        "\n",
        "def collect_files(root: str):\n",
        "    out = []\n",
        "    for p in glob.glob(root + \"/**/*\", recursive=True):\n",
        "        # Early continues reduce indentation bugs and keep logic clear.\n",
        "        if not os.path.isfile(p):\n",
        "            continue\n",
        "        if is_excluded(p):\n",
        "            continue\n",
        "\n",
        "        path = Path(p)\n",
        "        ext = path.suffix.lower()\n",
        "        name = path.name.lower()\n",
        "\n",
        "        # Include known extensions plus special filenames (Makefile, Dockerfile)\n",
        "        if ext in INCLUDE_EXT or name in {\"makefile\", \"dockerfile\"}:\n",
        "            if os.path.getsize(p) <= MAX_FILE_BYTES:\n",
        "                out.append(p)\n",
        "\n",
        "    return sorted(out)\n",
        "\n",
        "\n",
        "repo_files = {name: collect_files(path) for name, path in repo_dirs.items()}\n",
        "for name, flist in repo_files.items():\n",
        "    print(name, \"files:\", len(flist))\n",
        "\n"
      ],
      "metadata": {
        "id": "9bJaMCqNbF6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 3 — Build per-repo maps (tree + entrypoint hints)\n",
        "# ----------------------------------------------------------\n",
        "# Repo maps act like a \"table of contents\" for the model.\n",
        "# This reduces flailing and gives consistent orientation across repos.\n",
        "# ==========================================================\n",
        "ENTRYPOINT_HINTS = [\n",
        "    r\"^main\\.(py|js|ts|go|rs|java|kt|cs)$\",\n",
        "    r\"^app\\.(py|js|ts)$\",\n",
        "    r\"^index\\.(js|ts)$\",\n",
        "    r\"^server\\.(js|ts|py)$\",\n",
        "    r\"^manage\\.py$\",\n",
        "    r\"^pom\\.xml$\",\n",
        "    r\"^build\\.gradle(\\.kts)?$\",\n",
        "    r\"^package\\.json$\",\n",
        "    r\"^Dockerfile$\",\n",
        "    r\"^Makefile$\",\n",
        "]\n",
        "\n",
        "\n",
        "def repo_tree(root: str, max_lines: int = 300) -> str:\n",
        "    lines = []\n",
        "    count = 0\n",
        "    rootp = Path(root)\n",
        "    for p in sorted(rootp.rglob(\"*\")):\n",
        "        if count >= max_lines:\n",
        "            lines.append(\"... (tree truncated)\")\n",
        "            break\n",
        "        if is_excluded(str(p)) or p.is_dir():\n",
        "            continue\n",
        "        lines.append(str(p).replace(root + \"/\", \"\"))\n",
        "        count += 1\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def find_entrypoints(files_list, repo_root: str):\n",
        "    hits = []\n",
        "    for fp in files_list:\n",
        "        rel = fp.replace(repo_root + \"/\", \"\")\n",
        "        base = Path(rel).name\n",
        "        for pat in ENTRYPOINT_HINTS:\n",
        "            if re.match(pat, base, flags=re.IGNORECASE):\n",
        "                hits.append(rel)\n",
        "                break\n",
        "    return hits[:25]\n",
        "\n",
        "\n",
        "REPO_MAPS = {}\n",
        "for name, root in repo_dirs.items():\n",
        "    tree = repo_tree(root)\n",
        "    eps = find_entrypoints(repo_files[name], root)\n",
        "\n",
        "    REPO_MAPS[name] = (\n",
        "        f\"REPO: {name}\\n\"\n",
        "        f\"REPO TREE (partial):\\n{tree}\\n\\n\"\n",
        "        f\"LIKELY ENTRYPOINTS / IMPORTANT FILES:\\n- \"\n",
        "        + \"\\n- \".join(eps if eps else [\"(none detected)\"])\n",
        "    )\n",
        "\n",
        "print(REPO_MAPS[list(REPO_MAPS.keys())[0]][:1600])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cNi38VDnb0jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELL 4 — Chunking (Tree-sitter when possible)\n",
        "# ----------------------------------------------------------\n",
        "# Why chunking matters:\n",
        "# - RAG retrieves *chunks*, not whole repos.\n",
        "# - Better chunks = better context = better answers.\n",
        "# Tree-sitter helps preserve natural boundaries (functions/classes).\n",
        "# Fallback chunking handles everything else safely.\n",
        "# ==========================================================\n",
        "from tree_sitter_languages import get_parser\n",
        "\n",
        "TS_LANG = {\n",
        "    \".py\": \"python\",\n",
        "    \".js\": \"javascript\",\n",
        "    \".ts\": \"typescript\",\n",
        "    \".jsx\": \"javascript\",\n",
        "    \".tsx\": \"tsx\",\n",
        "    \".java\": \"java\",\n",
        "    \".go\": \"go\",\n",
        "    \".rs\": \"rust\",\n",
        "    \".c\": \"c\",\n",
        "    \".h\": \"c\",\n",
        "    \".cpp\": \"cpp\",\n",
        "    \".hpp\": \"cpp\",\n",
        "    \".cs\": \"c_sharp\",\n",
        "    \".php\": \"php\",\n",
        "    \".rb\": \"ruby\",\n",
        "    \".swift\": \"swift\",\n",
        "    \".kt\": \"kotlin\",\n",
        "    \".scala\": \"scala\",\n",
        "}\n",
        "\n",
        "\n",
        "def read_text(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def fallback_chunk(text: str, chunk_size: int = 1400, overlap: int = 200):\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        chunks.append(text[i : i + chunk_size])\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def ts_chunk(code: str, ext: str, max_chars: int = 1600):\n",
        "    lang = TS_LANG.get(ext)\n",
        "    if not lang:\n",
        "        return None\n",
        "    try:\n",
        "        parser = get_parser(lang)\n",
        "        tree = parser.parse(bytes(code, \"utf8\"))\n",
        "        root = tree.root_node\n",
        "\n",
        "        chunks, buf = [], \"\"\n",
        "        for child in root.children:\n",
        "            seg = code[child.start_byte : child.end_byte]\n",
        "            if not seg.strip():\n",
        "                continue\n",
        "\n",
        "            if len(buf) + len(seg) > max_chars and buf.strip():\n",
        "                chunks.append(buf)\n",
        "                buf = seg\n",
        "            else:\n",
        "                buf += (\"\\n\" if buf else \"\") + seg\n",
        "\n",
        "        if buf.strip():\n",
        "            chunks.append(buf)\n",
        "\n",
        "        return chunks if chunks else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def chunk_file(path: str):\n",
        "    code = read_text(path)\n",
        "    if len(code.strip()) < 20:\n",
        "        return []\n",
        "\n",
        "    ext = Path(path).suffix.lower()\n",
        "    chunks = ts_chunk(code, ext)\n",
        "    if chunks is None:\n",
        "        chunks = fallback_chunk(code)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EfJUZHlcCsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CELL 5 — Build federated docs/metas across ALL repos\n",
        "# ----------------------------------------------------------\n",
        "# We create one global list of chunks and metadata:\n",
        "#   metas_all[i] tells you which repo/file/chunk docs_all[i] came from.\n",
        "# This is the key to federated search.\n",
        "# ==========================================================\n",
        "\n",
        "docs_all, metas_all = [], []\n",
        "\n",
        "for repo_name, root in repo_dirs.items():\n",
        "    for fp in repo_files[repo_name]:\n",
        "        rel = fp.replace(root + \"/\", \"\")\n",
        "        chunks = chunk_file(fp)\n",
        "        for i, ch in enumerate(chunks):\n",
        "            docs_all.append(ch)\n",
        "            metas_all.append({\"repo\": repo_name, \"file\": rel, \"chunk\": i})\n",
        "\n",
        "print(\"Total federated chunks:\", len(docs_all))\n",
        "print(\"Example meta:\", metas_all[0] if metas_all else None)\n",
        "\n"
      ],
      "metadata": {
        "id": "CRmEfkijcEq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 6 — Hybrid retrieval index (BM25 + Embeddings + FAISS)\n",
        "# ----------------------------------------------------------\n",
        "# BM25: best for exact strings (AuthService, route names, filenames, flags)\n",
        "# Embeddings: best for meaning (\"how does auth work\", paraphrases, intent)\n",
        "# We merge both candidate pools and score them into a final ranking.\n",
        "# ==========================================================\n",
        "import numpy as np\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "def tokenize_for_bm25(text: str):\n",
        "    # Language-agnostic-ish tokenization:\n",
        "    # identifiers, numbers, and a few common operators/delimiters\n",
        "    return re.findall(r\"[A-Za-z_][A-Za-z0-9_]{1,}|\\d+|==|!=|<=|>=|->|::|\\.\", text)\n",
        "\n",
        "\n",
        "bm25_corpus = [tokenize_for_bm25(d) for d in docs_all]\n",
        "bm25 = BM25Okapi(bm25_corpus)\n",
        "\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "emb = embed_model.encode(docs_all, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "faiss_index = faiss.IndexFlatIP(emb.shape[1])  # cosine similarity via normalized vectors\n",
        "faiss_index.add(emb)\n",
        "\n",
        "\n",
        "def hybrid_retrieve(query: str, k: int = 12, pool: int = 60, alpha: float = 0.55, repo: str = \"all\"):\n",
        "    \"\"\"Hybrid retrieval across multiple repos.\n",
        "\n",
        "    Args:\n",
        "      query: the question\n",
        "      k: number of final chunks returned\n",
        "      pool: candidate pool from each retrieval method\n",
        "      alpha: weight toward semantic (embeddings) vs lexical (BM25)\n",
        "      repo: 'all' for federated or a specific repo name\n",
        "    \"\"\"\n",
        "    q_tok = tokenize_for_bm25(query)\n",
        "    bm25_scores = bm25.get_scores(q_tok)\n",
        "    bm25_top = np.argsort(bm25_scores)[::-1][:pool]\n",
        "\n",
        "    q_vec = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    sem_scores, sem_ids = faiss_index.search(q_vec, pool)\n",
        "\n",
        "    candidates = set(bm25_top.tolist()) | set(sem_ids[0].tolist())\n",
        "\n",
        "    bm25_max = float(np.max(bm25_scores)) if len(bm25_scores) else 1.0\n",
        "    sem_map = {int(i): float(s) for i, s in zip(sem_ids[0], sem_scores[0])}\n",
        "\n",
        "    scored = []\n",
        "    for i in candidates:\n",
        "        m = metas_all[i]\n",
        "        if repo != \"all\" and m[\"repo\"] != repo:\n",
        "            continue\n",
        "\n",
        "        b = float(bm25_scores[i]) / (bm25_max if bm25_max > 0 else 1.0)\n",
        "        s = sem_map.get(int(i), 0.0)\n",
        "        score = (1 - alpha) * b + alpha * s\n",
        "        scored.append((score, int(i), b, s))\n",
        "\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    results = []\n",
        "    for score, idx, b, s in scored[:k]:\n",
        "        results.append(\n",
        "            {\n",
        "                \"score\": float(score),\n",
        "                \"bm25\": float(b),\n",
        "                \"sem\": float(s),\n",
        "                \"text\": docs_all[idx],\n",
        "                \"meta\": metas_all[idx],\n",
        "                \"id\": idx,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "Z008ZsVFcHHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 7 — Multi-repo browsing tools\n",
        "# ----------------------------------------------------------\n",
        "# These tools let you drive the exploration like an engineer:\n",
        "# - open_file for precise line-level citations\n",
        "# - list_tree to browse\n",
        "# - search_symbol and find_references for quick navigation\n",
        "# ==========================================================\n",
        "\n",
        "def list_repos():\n",
        "    return list(repo_dirs.keys())\n",
        "\n",
        "\n",
        "def list_tree(repo: str, prefix: str = \"\", limit: int = 200):\n",
        "    root = repo_dirs[repo]\n",
        "    prefix = prefix.strip().lstrip(\"/\")\n",
        "\n",
        "    matches = []\n",
        "    for f in repo_files[repo]:\n",
        "        rel = f.replace(root + \"/\", \"\")\n",
        "        if rel.startswith(prefix):\n",
        "            matches.append(rel)\n",
        "    return matches[:limit]\n",
        "\n",
        "\n",
        "def open_file(repo: str, path: str, start: int = 1, end: int = 200):\n",
        "    root = repo_dirs[repo]\n",
        "    full = os.path.join(root, path)\n",
        "    if not os.path.exists(full):\n",
        "        return f\"File not found: {repo}/{path}\"\n",
        "\n",
        "    lines = read_text(full).splitlines()\n",
        "    start = max(1, start)\n",
        "    end = min(len(lines), end)\n",
        "\n",
        "    return \"\\n\".join(f\"{i+1:4d} | {lines[i]}\" for i in range(start - 1, end))\n",
        "\n",
        "\n",
        "def search_symbol(repo: str, symbol: str, limit: int = 50):\n",
        "    root = repo_dirs[repo]\n",
        "    pat = re.compile(rf\"\\b{re.escape(symbol)}\\b\")\n",
        "\n",
        "    hits = []\n",
        "    for f in repo_files[repo]:\n",
        "        rel = f.replace(root + \"/\", \"\")\n",
        "        if pat.search(read_text(f)):\n",
        "            hits.append(rel)\n",
        "            if len(hits) >= limit:\n",
        "                break\n",
        "\n",
        "    return hits\n",
        "\n",
        "\n",
        "def find_references(repo: str, symbol: str, limit: int = 60):\n",
        "    root = repo_dirs[repo]\n",
        "    pat = re.compile(rf\"\\b{re.escape(symbol)}\\b\")\n",
        "\n",
        "    refs = []\n",
        "    for f in repo_files[repo]:\n",
        "        rel = f.replace(root + \"/\", \"\")\n",
        "        for i, line in enumerate(read_text(f).splitlines()):\n",
        "            if pat.search(line):\n",
        "                refs.append((rel, i + 1, line.strip()[:240]))\n",
        "                if len(refs) >= limit:\n",
        "                    return refs\n",
        "\n",
        "    return refs\n",
        "\n",
        "\n",
        "print(\"Repos:\", list_repos())\n",
        "\n"
      ],
      "metadata": {
        "id": "Px8K49STcJJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 8 — Load LLM (runs in Colab)\n",
        "# ----------------------------------------------------------\n",
        "# Use a reasonably capable instruct model.\n",
        "# - Mistral 7B 4-bit is often OK on a Colab T4/A10\n",
        "# - If you hit memory issues, switch to TinyLlama.\n",
        "# ==========================================================\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# If VRAM issues:\n",
        "# MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "gen = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
        "print(\"Loaded:\", MODEL_NAME)\n",
        "\n"
      ],
      "metadata": {
        "id": "BsnmM8zBcKc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 9 — Federated Q&A with confidence guardrails\n",
        "# ----------------------------------------------------------\n",
        "# The model must cite evidence.\n",
        "# If confidence is low, it asks follow-ups and suggests which files to open.\n",
        "# This reduces hallucinations and encourages inspectability.\n",
        "# ==========================================================\n",
        "SYSTEM = \"\"\"You are a senior software engineer. You can search across multiple repositories.\n",
        "Rules:\n",
        "- Use ONLY the provided CONTEXT + REPO MAPS + TOOL OUTPUTS for factual claims.\n",
        "- Cite repo/file paths in evidence bullets.\n",
        "- If context is insufficient, say what to inspect next and ask 1-2 targeted questions.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def confidence_from_hits(hits):\n",
        "    # Simple heuristic: top score + how quickly the relevance drops.\n",
        "    if not hits:\n",
        "        return 0.0\n",
        "    top = hits[0][\"score\"]\n",
        "    third = hits[2][\"score\"] if len(hits) > 2 else hits[-1][\"score\"]\n",
        "    gap = top - third\n",
        "    conf = max(0.0, min(1.0, top * 0.9 + (1.0 - min(1.0, gap * 2)) * 0.1))\n",
        "    return float(conf)\n",
        "\n",
        "\n",
        "def build_context(question: str, repo: str, k: int):\n",
        "    hits = hybrid_retrieve(question, repo=repo, k=k)\n",
        "    blocks = []\n",
        "\n",
        "    for h in hits:\n",
        "        m = h[\"meta\"]\n",
        "        blocks.append(\n",
        "            f\"[{m['repo']}/{m['file']} :: chunk {m['chunk']} :: score {h['score']:.3f} \"\n",
        "            f\"(bm25 {h['bm25']:.2f}, sem {h['sem']:.2f})]\\n\"\n",
        "            f\"{h['text']}\"\n",
        "        )\n",
        "\n",
        "    return hits, \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "\n",
        "def ask_repo(question: str, repo: str = \"all\", k: int = 12, max_new_tokens: int = 550):\n",
        "    \"\"\"Ask a question scoped to a repo or federated across all repos.\n",
        "\n",
        "    repo:\n",
        "      - 'all' searches everything\n",
        "      - or a specific repo name (e.g., 'repoA')\n",
        "    \"\"\"\n",
        "    hits, context = build_context(question, repo=repo, k=k)\n",
        "    conf = confidence_from_hits(hits)\n",
        "\n",
        "    # Provide the model a map. For federated, include all maps.\n",
        "    if repo == \"all\":\n",
        "        maps = \"\\n\\n\".join(REPO_MAPS[r] for r in REPO_MAPS.keys())\n",
        "    else:\n",
        "        maps = REPO_MAPS.get(repo, f\"REPO: {repo} (map not found)\")\n",
        "\n",
        "    prompt = f\"\"\"{SYSTEM}\n",
        "\n",
        "REPO MAPS:\n",
        "{maps}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "CONTEXT (top {k}, repo={repo}):\n",
        "{context}\n",
        "\n",
        "CONFIDENCE:\n",
        "{conf:.2f}\n",
        "\n",
        "ANSWER FORMAT:\n",
        "- Direct answer\n",
        "- Evidence bullets (repo/file paths)\n",
        "- If confidence < 0.55: ask 1-2 clarifying questions + suggest next files/tools to inspect\n",
        "\"\"\"\n",
        "\n",
        "    out = gen(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    return out[len(prompt) :].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "2_L1YK7bcN3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================================\n",
        "# CELL 10 — Examples (run when ready)\n",
        "# ----------------------------------------------------------\n",
        "# 1) Federated question across all repos\n",
        "# print(ask_repo(\"Which repo implements authentication, and where?\", repo=\"all\"))\n",
        "\n",
        "#\n",
        "# 2) Repo-scoped question\n",
        "# print(ask_repo(\"Where is the main entrypoint and how does the app start?\", repo=\"repoA\"))\n",
        "#\n",
        "# 3) Use tools for precise investigation\n",
        "# print(search_symbol(\"repoA\", \"AuthService\"))\n",
        "# print(open_file(\"repoA\", \"README.md\", 1, 120))\n",
        "# print(find_references(\"repoB\", \"TODO\"))\n",
        "\n",
        "# moonshot\n",
        "print(ask_repo(\"What is the best functionality I can come up with if you combine these repos and use a common functionality for good of Testing?\", repo=\"all\"))\n",
        "# ==========================================================\n"
      ],
      "metadata": {
        "id": "qzHJojsHfvVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}