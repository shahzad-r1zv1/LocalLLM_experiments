{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzad-r1zv1/LocalLLM_experiments/blob/main/7s.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pandas tqdm matplotlib reportlab python-pptx requests\n"
      ],
      "metadata": {
        "id": "xlMSHEIBQIpe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================== ONE-CELL 7S + OLLAMA (COLAB, WRC EDITION) ==============================\n",
        "# Installs Ollama, pulls up to 3 models, runs selected 7S prompts across models (WRC wording),\n",
        "# consolidates results, and saves JSON/Markdown outputs in /content.\n",
        "\n",
        "import os, subprocess, shlex, time, requests, json, re, textwrap, datetime\n",
        "from typing import Dict, List\n",
        "\n",
        "# ---------------------------\n",
        "# 0) BASIC CONFIG (EDIT THESE)\n",
        "# ---------------------------\n",
        "# Organization inputs used to fill prompt placeholders\n",
        "ORG = {\n",
        "    \"name\": \"QA Automation Org\",\n",
        "    \"industry\": \"Software / DevTools\",\n",
        "    \"size_employees\": 180,\n",
        "    \"annual_revenue\": \"N/A (cost center)\",\n",
        "    \"locations\": \"Toronto; Remote\",\n",
        "    \"current_context\": \"Transitioning from phased QA to DevSecOps; aiming for predictive quality and CI/CD-first culture.\",\n",
        "    \"key_challenges\": \"Inconsistent test coverage; siloed defect data; unclear decision rights; skills gap in pipeline engineering.\",\n",
        "    \"recent_changes\": \"Trunk-based dev in 2 squads; feature flags/canary pilots; Playwright + contract tests on 2 critical APIs.\",\n",
        "    # Optional fields for specific prompts (leave as \"\" if unknown)\n",
        "    \"strategy_statement\": \"Ship faster with confidence via automated quality gates and proactive risk analytics.\",\n",
        "    \"strategic_goals\": \"12 months: 70% automated coverage; 24m: 95% critical path; MTTR<1hr; DORA elite.\",\n",
        "    \"kpis\": \"DORA metrics, escaped defect rate, mean time to detect, e2e reliability SLOs.\",\n",
        "    \"market_position\": \"Internal enablement platform within a large enterprise.\",\n",
        "    \"competitive_advantages\": \"Domain expertise; test data virtualization; unified telemetry.\",\n",
        "    \"target_segments\": \"Product teams building microservices/APIs.\",\n",
        "    \"value_proposition\": \"Fewer incidents, faster releases, higher trust in automation.\",\n",
        "    \"org_chart\": \"VP Eng ‚Üí Dir QA/Platform ‚Üí QA Enablement, SDET, Observability pods; squads map to product lines.\",\n",
        "    \"layers\": \"VP ‚Üí Director ‚Üí Managers ‚Üí ICs (4 layers)\",\n",
        "    \"span_of_control\": \"Managers: 6-8 ICs avg\",\n",
        "    \"decision_rights\": \"Product owns scope; QA owns quality standards; Dev owns implementation; shared release gates.\",\n",
        "    \"xfunc\": \"Release Council; Reliability Guild; Architecture Forum.\",\n",
        "    \"geo\": \"Toronto hub + distributed remote\",\n",
        "    \"restructuring\": \"Evolving towards platform team model; decoupling test infra from app squads.\"\n",
        "}\n",
        "\n",
        "# Which prompts to run (choose from keys below)\n",
        "RUN_PROMPTS = [\n",
        "    \"PROMPT_1_FULL_7S\",\n",
        "    \"PROMPT_2_STRATEGY\",\n",
        "    \"PROMPT_3_STRUCTURE\",\n",
        "    \"PROMPT_4_SYSTEMS\",\n",
        "    \"PROMPT_5_SHARED_VALUES\",\n",
        "    \"PROMPT_6_SKILLS\",\n",
        "    \"PROMPT_7_STYLE\",\n",
        "    \"PROMPT_8_STAFF\",\n",
        "    \"PROMPT_9_ALIGNMENT\",\n",
        "    \"PROMPT_10_CHANGE\",\n",
        "    \"PROMPT_11_DIGITAL\",\n",
        "    \"PROMPT_12_BENCH\",\n",
        "    \"PROMPT_13_GAPS\",\n",
        "    \"PROMPT_14_INTEGRATION\",\n",
        "    \"PROMPT_15_EXEC_SUMMARY\",\n",
        "    \"MEGA_PROMPT\"\n",
        "]"
      ],
      "metadata": {
        "id": "Az5Pt0ZvjqiY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to try pulling (we‚Äôll use what actually succeeds), \"qwen3:8b\"\n",
        "# REQUESTED_MODELS = [\"qwen3\", \"mistral\", \"llama3\"]\n",
        "#REQUESTED_MODELS = [\"qwen3\", \"phi3\", \"llama3\"]\n",
        "# Common alias fixes (Ollama doesn't have \"gemma3\")\n",
        "# MODEL_ALIASES = {\n",
        "#     \"gemma3\": \"gemma2:9b-instruct\",\n",
        "#     \"gemma2-9b\": \"gemma2:9b-instruct\",\n",
        "#     \"gemma2-2b\": \"gemma2:2b-instruct\",\n",
        "#     \"gemma7b\": \"gemma:7b-instruct\",\n",
        "#     \"llama3.1\": \"llama3.1\",\n",
        "#      # pass-through example\n",
        "# }\n",
        "\n",
        "# # --- Pull models asynchronously ---\n",
        "# models = [\"mistral\", \"llama3\", \"gemma2:2b-instruct\"]  # use gemma2:2b or 9b instead of gemma3\n",
        "# models = [\"qwen3\", \"phi3\", \"llama3\"]\n",
        "\n",
        "MODEL_ALIASES = {\n",
        "    \"gemma3\": \"gemma2:9b-instruct\",\n",
        "    \"gemma2\": \"gemma2:2b-instruct\",\n",
        "    \"llama3\": \"llama3\",\n",
        "    \"llama3.1\": \"llama3.1\",\n",
        "    \"mistral\": \"mistral\",\n",
        "    \"phi4\":\"phi4\",\n",
        "    \"qwen3\" : \"qwen3:14b\",\n",
        "    \"gemma7b\": \"gemma:7b-instruct\",\n",
        "}\n",
        "REQUESTED_MODELS = [\"mistral\", \"llama3\", \"phi4\"]\n",
        "REQUESTED_MODELS = [MODEL_ALIASES.get(m.lower(), m) for m in REQUESTED_MODELS]\n",
        "\n",
        "# Output paths\n",
        "OUT_DIR = \"/content\"\n",
        "BUNDLE_JSON = os.path.join(OUT_DIR, \"7S_bundle.json\")\n",
        "REPORT_MD   = os.path.join(OUT_DIR, \"7S_report.md\")"
      ],
      "metadata": {
        "id": "Kgshbw4ajrmw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# üöÄ OLLAMA INSTALL / START / SMART MODEL PULLER  (with speed + restart)\n",
        "# ===============================================================\n",
        "import subprocess, shlex, requests, time, os, sys, re, math\n",
        "\n",
        "def run(cmd, check=True, quiet=False):\n",
        "    if isinstance(cmd, str):\n",
        "        cmd = shlex.split(cmd)\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if not quiet: print(p.stdout.strip())\n",
        "    if check and p.returncode != 0:\n",
        "        raise RuntimeError(f\"CMD FAIL: {' '.join(cmd)}\\n----\\n{p.stdout}\")\n",
        "    return p.stdout.strip()\n",
        "\n",
        "def ollama_installed():\n",
        "    try:\n",
        "        out = run(\"ollama --version\", check=False, quiet=True)\n",
        "        return \"ollama\" in out.lower()\n",
        "    except Exception: return False\n",
        "\n",
        "def ollama_running():\n",
        "    try:\n",
        "        r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=3)\n",
        "        return r.status_code == 200\n",
        "    except Exception: return False\n",
        "\n",
        "# ------------------- INSTALL -------------------\n",
        "if ollama_installed():\n",
        "    print(\"‚úÖ Ollama already installed.\")\n",
        "else:\n",
        "    print(\"üì¶ Installing Ollama (with progress)...\")\n",
        "    proc = subprocess.Popen(\n",
        "        \"bash -lc 'curl -fSL https://ollama.com/install.sh | sh'\",\n",
        "        shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT\n",
        "    )\n",
        "    for line in iter(proc.stdout.readline, ''):\n",
        "        sys.stdout.write(line); sys.stdout.flush()\n",
        "    proc.wait()\n",
        "    print(\"‚úÖ Installation complete.\" if proc.returncode == 0 else \"‚ö†Ô∏è Installation may have issues!\")\n",
        "\n",
        "# ------------------- START SERVER -------------------\n",
        "if ollama_running():\n",
        "    print(\"‚úÖ Ollama already running.\")\n",
        "else:\n",
        "    print(\"üöÄ Starting ollama serve‚Ä¶\")\n",
        "    run(\"bash -lc 'pkill -f \\\"ollama serve\\\" || true'\", check=False, quiet=True)\n",
        "    logf = open(\"/tmp/ollama.log\", \"w\")\n",
        "    proc = subprocess.Popen([\"ollama\",\"serve\"], stdout=logf, stderr=subprocess.STDOUT, text=True)\n",
        "    for _ in range(60):\n",
        "        if ollama_running(): break\n",
        "        print(\".\", end=\"\", flush=True); time.sleep(2)\n",
        "    print(\"\\n‚úÖ Ollama API ready at http://127.0.0.1:11434\")\n",
        "\n",
        "# ------------------- SMART MODEL PULLER -------------------\n",
        "def model_exists(m):\n",
        "    try:\n",
        "        r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=5)\n",
        "        if r.status_code==200:\n",
        "            names=[x[\"name\"] for x in r.json().get(\"models\",[])]\n",
        "            return any(m.startswith(x) or x.startswith(m) for x in names)\n",
        "    except: return False\n",
        "    return False\n",
        "\n",
        "def pull_model(m, max_retries=2, stall_threshold=0.10, stall_time=300):\n",
        "    \"\"\"\n",
        "    Pull model with live progress, MB/s estimate, smart stall recovery,\n",
        "    and periodic heartbeat for Colab environments.\n",
        "    \"\"\"\n",
        "    MODEL_SIZES = {\n",
        "        \"mistral\": 4200, \"llama3\": 8000, \"gemma2:9b-instruct\": 9000,\n",
        "        \"phi3\": 3800, \"gemma2:2b\": 2200\n",
        "    }\n",
        "    total_mb = MODEL_SIZES.get(m.lower(), 4000)\n",
        "    attempt = 0\n",
        "\n",
        "    while attempt <= max_retries:\n",
        "        attempt += 1\n",
        "        if model_exists(m):\n",
        "            print(f\"‚úÖ {m} already available.\")\n",
        "            return True\n",
        "\n",
        "        print(f\"\\n‚¨áÔ∏è Pulling {m} (attempt {attempt}/{max_retries+1})...\")\n",
        "        p = subprocess.Popen([\"ollama\", \"pull\", m],\n",
        "                             stdout=subprocess.PIPE,\n",
        "                             stderr=subprocess.STDOUT,\n",
        "                             text=True, bufsize=1)\n",
        "\n",
        "        prog = re.compile(r\"(\\d+)%\")\n",
        "        last_pct, last_time = 0, time.time()\n",
        "        speed_window = []\n",
        "        last_update = time.time()\n",
        "        last_heartbeat = time.time()\n",
        "        stalled = False\n",
        "\n",
        "        try:\n",
        "            for line in iter(p.stdout.readline, \"\"):\n",
        "                line = line.strip()\n",
        "                match = prog.search(line)\n",
        "                now = time.time()\n",
        "\n",
        "                if match:\n",
        "                    pct = int(match.group(1))\n",
        "                    dt = max(now - last_time, 0.1)\n",
        "                    dp = pct - last_pct\n",
        "\n",
        "                    if dp > 0:\n",
        "                        inst_speed = dp / dt\n",
        "                        speed_window.append(inst_speed)\n",
        "                        if len(speed_window) > 5:\n",
        "                            speed_window.pop(0)\n",
        "                        avg_speed = sum(speed_window) / len(speed_window)\n",
        "                        last_time, last_pct = now, pct\n",
        "                    else:\n",
        "                        avg_speed = sum(speed_window) / len(speed_window) if speed_window else 0\n",
        "\n",
        "                    mb_speed = (avg_speed / 100.0) * total_mb / max(dt, 1)\n",
        "                    eta = min(((100 - pct) / max(avg_speed, 0.01)), 9999)\n",
        "\n",
        "                    bar = f\"[{'='*(pct//5):<20}]\"\n",
        "                    sys.stdout.write(\n",
        "                        f\"\\rüì¶ {m:<18} {bar} {pct:3d}% | {avg_speed:4.2f}%/s | {mb_speed:5.2f} MB/s | ETA {eta:4.0f}s\"\n",
        "                    )\n",
        "                    sys.stdout.flush()\n",
        "                    last_update = now\n",
        "\n",
        "                # Heartbeat if quiet for a while\n",
        "                elif now - last_heartbeat > 90:\n",
        "                    sys.stdout.write(\" üíì\")\n",
        "                    sys.stdout.flush()\n",
        "                    last_heartbeat = now\n",
        "\n",
        "                # Stall detection\n",
        "                if now - last_update > stall_time:\n",
        "                    print(f\"\\n‚ö†Ô∏è {m} stalled (no update for {stall_time}s). Restarting...\")\n",
        "                    p.kill()\n",
        "                    stalled = True\n",
        "                    break\n",
        "\n",
        "            p.wait()\n",
        "            print()  # newline after progress bar\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Error while pulling {m}: {e}\")\n",
        "            p.kill()\n",
        "            stalled = True\n",
        "\n",
        "        if not stalled and p.returncode == 0:\n",
        "            print(f\"‚úÖ {m} pull complete.\")\n",
        "            return True\n",
        "        elif attempt <= max_retries:\n",
        "            print(f\"üîÅ Retrying {m} in 10 s...\")\n",
        "            time.sleep(10)\n",
        "        else:\n",
        "            print(f\"‚ùå {m} failed (exit {p.returncode}).\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------- EXECUTE PULL -------------------\n",
        "REQUESTED_MODELS = [\"mistral\", \"llama3\", \"gemma2\"]\n",
        "print(\"\\nüì• Pulling models via running daemon...\\n\")\n",
        "\n",
        "for m in REQUESTED_MODELS:\n",
        "    pull_model(m)\n",
        "\n",
        "print(\"\\nüìã Models available now:\")\n",
        "!ollama list\n"
      ],
      "metadata": {
        "id": "s4_GiZ4epPjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9bb193-48b3-4723-cafc-ab13bac2d95b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ollama already installed.\n",
            "üöÄ Starting ollama serve‚Ä¶\n",
            ".\n",
            "‚úÖ Ollama API ready at http://127.0.0.1:11434\n",
            "\n",
            "üì• Pulling models via running daemon...\n",
            "\n",
            "‚úÖ mistral already available.\n",
            "‚úÖ llama3 already available.\n",
            "‚úÖ gemma2 already available.\n",
            "\n",
            "üìã Models available now:\n",
            "NAME              ID              SIZE      MODIFIED       \n",
            "mistral:latest    6577803aa9a0    4.4 GB    8 minutes ago     \n",
            "gemma2:latest     ff02c3702f32    5.4 GB    35 minutes ago    \n",
            "llama3:latest     365c0bd3c000    4.7 GB    43 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Base endpoints\n",
        "BASE = \"http://127.0.0.1:11434\"\n",
        "TAGS = f\"{BASE}/api/tags\"\n",
        "CHAT = f\"{BASE}/api/chat\"\n",
        "GEN  = f\"{BASE}/api/generate\"\n",
        "print(f\"üåê Connected to Ollama at {BASE}\")\n",
        "PRIMARY_MODEL = REQUESTED_MODELS[0]"
      ],
      "metadata": {
        "id": "86JKChI44VKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cabd07-1892-418c-c751-df138e02f81b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Connected to Ollama at http://127.0.0.1:11434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 1Ô∏è‚É£ INSTALL & START OLLAMA (idempotent)\n",
        "# ---------------------------\n",
        "import subprocess, shlex, requests, time, os\n",
        "\n",
        "def run(cmd, check=True, quiet=False):\n",
        "    \"\"\"Run a shell command and optionally suppress output.\"\"\"\n",
        "    if isinstance(cmd, str):\n",
        "        cmd = shlex.split(cmd)\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if not quiet:\n",
        "        print(p.stdout.strip())\n",
        "    if check and p.returncode != 0:\n",
        "        raise RuntimeError(f\"CMD FAIL: {' '.join(cmd)}\\n----\\n{p.stdout}\")\n",
        "    return p.stdout.strip()\n",
        "\n",
        "def ollama_installed():\n",
        "    try:\n",
        "        out = run(\"ollama --version\", check=False, quiet=True)\n",
        "        return \"ollama\" in out.lower() or \"version\" in out.lower()\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def ollama_running():\n",
        "    try:\n",
        "        r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=3)\n",
        "        return r.status_code == 200\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# 1. Install Ollama if needed\n",
        "if ollama_installed():\n",
        "    print(\"‚úÖ Ollama already installed.\")\n",
        "else:\n",
        "    print(\"üì¶ Installing Ollama‚Ä¶\")\n",
        "    import subprocess, shlex, sys, time\n",
        "\n",
        "    # Use non-silent curl (remove -s) so progress bar appears\n",
        "    install_cmd = \"bash -lc 'curl -fSL https://ollama.com/install.sh | sh'\"\n",
        "\n",
        "    # Stream output in real time so you can see download progress\n",
        "    proc = subprocess.Popen(install_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "    last_update = time.time()\n",
        "    for line in iter(proc.stdout.readline, ''):\n",
        "        if line.strip():\n",
        "            # throttle slightly so Colab output remains responsive\n",
        "            if time.time() - last_update > 0.2:\n",
        "                sys.stdout.write(line)\n",
        "                sys.stdout.flush()\n",
        "                last_update = time.time()\n",
        "\n",
        "    proc.wait()\n",
        "    if proc.returncode == 0:\n",
        "        print(\"‚úÖ Ollama installation complete.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Installation exited with code {proc.returncode}. Check above logs.\")\n",
        "\n",
        "    run(\"bash -lc 'curl -fsSL https://ollama.com/install.sh | sh'\", check=False, quiet=True)\n",
        "    print(\"‚úÖ Installation complete.\")\n",
        "\n",
        "# 2. Start Ollama service (if not already running)\n",
        "if ollama_running():\n",
        "    print(\"‚úÖ Ollama service already running.\")\n",
        "else:\n",
        "    print(\"üöÄ Starting ollama serve‚Ä¶\")\n",
        "    run(\"bash -lc 'pkill -f \\\"ollama serve\\\" || true'\", check=False, quiet=True)\n",
        "    log_path = \"/tmp/ollama.log\"\n",
        "    logf = open(log_path, \"w\")\n",
        "    proc = subprocess.Popen([\"ollama\", \"serve\"], stdout=logf, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "    print(\"‚è≥ Waiting for Ollama to become ready\", end=\"\")\n",
        "    ready = False\n",
        "    for _ in range(60):\n",
        "        if ollama_running():\n",
        "            ready = True\n",
        "            break\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(2)\n",
        "    print()\n",
        "\n",
        "    if not ready:\n",
        "        print(\"‚ö†Ô∏è Ollama did not start. Showing last 30 log lines:\")\n",
        "        try:\n",
        "            print(\"\".join(open(log_path, \"r\", errors=\"ignore\").readlines()[-30:]))\n",
        "        except Exception:\n",
        "            pass\n",
        "        raise SystemExit(\"‚ùå Cannot continue without Ollama.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Ollama API ready at http://127.0.0.1:11434\")\n",
        "\n",
        "# Base endpoints\n",
        "BASE = \"http://127.0.0.1:11434\"\n",
        "TAGS = f\"{BASE}/api/tags\"\n",
        "CHAT = f\"{BASE}/api/chat\"\n",
        "GEN  = f\"{BASE}/api/generate\"\n",
        "print(f\"üåê Connected to Ollama at {BASE}\")\n"
      ],
      "metadata": {
        "id": "piFQ0u-Gj0NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f38211d-55de-430c-ac0e-741f7f2dd88c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ollama already installed.\n",
            "‚úÖ Ollama service already running.\n",
            "üåê Connected to Ollama at http://127.0.0.1:11434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 3) PROMPT TEMPLATES (WRC wording)\n",
        "# ---------------------------\n",
        "def fill(s: str, kv: Dict[str,str]) -> str:\n",
        "    # simple replacement of [FIELD] with kv.get(field_lower)\n",
        "    def rep(m):\n",
        "        key = m.group(1).strip().lower().replace(\" \", \"_\")\n",
        "        return str(kv.get(key, f\"N/A: {m.group(0)}\"))\n",
        "    return re.sub(r\"\\[([^\\]]+)\\]\", rep, s)\n",
        "\n",
        "PROMPTS = {\n",
        "\"PROMPT_1_FULL_7S\": \"\"\"Act as a WRC (World Renowned Consultancy) consultant conducting a comprehensive 7S analysis.\n",
        "INPUTS NEEDED:\n",
        "- Company/Organization name: [NAME]\n",
        "- Industry: [INDUSTRY]\n",
        "- Company size (employees): [NUMBER]\n",
        "- Annual revenue: [REVENUE]\n",
        "- Geographic presence: [LOCATIONS]\n",
        "- Current business context: [BRIEF DESCRIPTION]\n",
        "- Key challenges facing the organization: [LIST 3-5]\n",
        "- Recent major changes or initiatives: [DESCRIBE]\n",
        "Analyze all seven elements:\n",
        "1. Strategy (competitive positioning, strategic priorities, value proposition)\n",
        "2. Structure (org design, reporting lines, decision rights)\n",
        "3. Systems (processes, IT systems, workflows)\n",
        "4. Shared Values (culture, core beliefs, mission/vision)\n",
        "5. Style (leadership approach, management practices)\n",
        "6. Staff (workforce composition, capabilities, engagement)\n",
        "7. Skills (organizational competencies, competitive advantages)\n",
        "OUTPUT FORMAT:\n",
        "- Executive summary with key findings\n",
        "- Detailed analysis of each S (current state assessment)\n",
        "- Interconnections and dependencies between elements\n",
        "- Alignment score (1-10) for each element pair\n",
        "- Top 5 misalignments requiring attention\n",
        "- Recommendations prioritized by impact and feasibility\n",
        "\"\"\",\n",
        "\"PROMPT_2_STRATEGY\": \"\"\"Evaluate the clarity and effectiveness of our organizational strategy using the 7S model.\n",
        "INPUTS NEEDED:\n",
        "- Current strategy statement: [PROVIDE]\n",
        "- Strategic goals (3-5 year): [LIST]\n",
        "- Key performance indicators: [LIST]\n",
        "- Market position: [DESCRIBE]\n",
        "- Competitive advantages: [LIST]\n",
        "- Target customer segments: [DESCRIBE]\n",
        "- Value proposition: [STATEMENT]\n",
        "Assess our strategy by examining:\n",
        "1. Clarity and communication throughout organization\n",
        "2. Alignment with market opportunities\n",
        "3. Differentiation from competitors\n",
        "4. Resource allocation alignment\n",
        "5. Measurability and tracking mechanisms\n",
        "6. Connection to other 6 S elements\n",
        "OUTPUT:\n",
        "- Strategy effectiveness score (1-10) with justification\n",
        "- Strengths and weaknesses analysis\n",
        "- Gaps between stated and actual strategy\n",
        "- Impact assessment on other 6 S elements\n",
        "- 5 specific recommendations to strengthen strategy\n",
        "- Communication plan to improve strategy understanding\n",
        "\"\"\",\n",
        "\"PROMPT_3_STRUCTURE\": \"\"\"Analyze our organizational structure's alignment with strategic goals.\n",
        "INPUTS NEEDED:\n",
        "- Current org chart: [DESCRIBE HIERARCHY]\n",
        "- Number of management layers: [NUMBER]\n",
        "- Span of control averages: [NUMBERS]\n",
        "- Decision-making authority levels: [DESCRIBE]\n",
        "- Cross-functional teams/committees: [LIST]\n",
        "- Geographic/divisional structure: [DESCRIBE]\n",
        "- Recent restructuring efforts: [DESCRIBE IF ANY]\n",
        "Evaluate:\n",
        "1. Structure-strategy fit\n",
        "2. Decision-making speed and effectiveness\n",
        "3. Communication flow efficiency\n",
        "4. Collaboration barriers\n",
        "5. Duplication or gaps in responsibilities\n",
        "6. Flexibility for future growth\n",
        "OUTPUT:\n",
        "- Structure effectiveness rating with evidence\n",
        "- Organizational design recommendations\n",
        "- Proposed org chart modifications\n",
        "- Impact analysis on systems and staff\n",
        "- Implementation roadmap for structural changes\n",
        "- Risk assessment of proposed changes\n",
        "\"\"\",\n",
        "\"PROMPT_4_SYSTEMS\": \"\"\"Assess the effectiveness of operational and management systems.\n",
        "INPUTS NEEDED:\n",
        "- Core business processes: [LIST]\n",
        "- IT systems and platforms: [LIST]\n",
        "- Performance management systems: [DESCRIBE]\n",
        "- Financial/budgeting processes: [DESCRIBE]\n",
        "- Quality control systems: [DESCRIBE]\n",
        "- Communication systems: [DESCRIBE]\n",
        "- Decision-making processes: [DESCRIBE]\n",
        "- Knowledge management systems: [DESCRIBE]\n",
        "Analyze:\n",
        "1. Process efficiency and effectiveness\n",
        "2. System integration and data flow\n",
        "3. Automation opportunities\n",
        "4. Performance tracking capabilities\n",
        "5. User satisfaction and adoption\n",
        "6. Alignment with strategic objectives\n",
        "OUTPUT:\n",
        "- Systems maturity assessment (1-5 scale per system)\n",
        "- Critical system gaps and redundancies\n",
        "- Process optimization opportunities\n",
        "- Technology upgrade recommendations\n",
        "- Implementation priority matrix\n",
        "- ROI estimates for system improvements\n",
        "\"\"\",\n",
        "\"PROMPT_5_SHARED_VALUES\": \"\"\"Identify and evaluate core shared values driving culture and decision-making.\n",
        "INPUTS NEEDED:\n",
        "- Stated mission/vision/values: [PROVIDE]\n",
        "- Employee survey results: [KEY FINDINGS]\n",
        "- Leadership behaviors observed: [EXAMPLES]\n",
        "- Decision-making patterns: [DESCRIBE]\n",
        "- Customer feedback themes: [SUMMARIZE]\n",
        "- Internal communication samples: [PROVIDE EXAMPLES]\n",
        "- Recognition and reward criteria: [LIST]\n",
        "Examine:\n",
        "1. Stated vs. lived values gap analysis\n",
        "2. Values alignment across hierarchy\n",
        "3. Values impact on behaviors\n",
        "4. Cultural strengths and toxicities\n",
        "5. Values-strategy alignment\n",
        "6. Employee value perception\n",
        "OUTPUT:\n",
        "- Core values identification (top 5 actual vs. stated)\n",
        "- Cultural health score with supporting evidence\n",
        "- Values-behavior alignment matrix\n",
        "- Cultural transformation requirements\n",
        "- Values reinforcement action plan\n",
        "- Measurement framework for cultural change\n",
        "\"\"\",\n",
        "\"PROMPT_6_SKILLS\": \"\"\"Evaluate critical skills and competencies across the organization.\n",
        "INPUTS NEEDED:\n",
        "- Current workforce skills inventory: [CATEGORIES]\n",
        "- Strategic capability requirements: [LIST]\n",
        "- Competitor capabilities: [BENCHMARK DATA]\n",
        "- Training and development programs: [DESCRIBE]\n",
        "- Performance review data: [KEY METRICS]\n",
        "- Skills gaps identified by managers: [LIST]\n",
        "- Future skill requirements (3-5 years): [ANTICIPATE]\n",
        "Assess:\n",
        "1. Current vs. required skills gaps\n",
        "2. Core competency strengths\n",
        "3. Competitive skill advantages/disadvantages\n",
        "4. Skills development effectiveness\n",
        "5. Knowledge transfer mechanisms\n",
        "6. Innovation and adaptation capabilities\n",
        "OUTPUT:\n",
        "- Skills heat map (current vs. required)\n",
        "- Critical skills gap analysis with risk levels\n",
        "- Competency development roadmap\n",
        "- Make/buy/partner talent decisions\n",
        "- L&D investment recommendations\n",
        "- Skills KPIs and tracking mechanisms\n",
        "\"\"\",\n",
        "\"PROMPT_7_STYLE\": \"\"\"Analyze the dominant leadership style and its impact on performance.\n",
        "INPUTS NEEDED:\n",
        "- Leadership team composition: [DESCRIBE]\n",
        "- Leadership assessment data: [IF AVAILABLE]\n",
        "- Employee engagement scores: [PROVIDE]\n",
        "- Decision-making examples: [3-5 CASES]\n",
        "- Communication patterns: [DESCRIBE]\n",
        "- Change management approaches: [EXAMPLES]\n",
        "- Succession planning status: [DESCRIBE]\n",
        "Evaluate:\n",
        "1. Predominant leadership styles\n",
        "2. Leadership effectiveness metrics\n",
        "3. Style-strategy alignment\n",
        "4. Leadership impact on culture\n",
        "5. Decision-making patterns\n",
        "6. Leadership development needs\n",
        "OUTPUT:\n",
        "- Leadership style profile with strengths/weaknesses\n",
        "- Leadership effectiveness score (1-10)\n",
        "- Style-situation fit analysis\n",
        "- Leadership development priorities\n",
        "- Succession planning recommendations\n",
        "- Leadership behavior change roadmap\n",
        "\"\"\",\n",
        "\"PROMPT_8_STAFF\": \"\"\"Review workforce composition, recruitment, and retention strategies.\n",
        "INPUTS NEEDED:\n",
        "- Total headcount and demographics: [PROVIDE]\n",
        "- Organizational structure by function: [BREAKDOWN]\n",
        "- Turnover rates by level/function: [DATA]\n",
        "- Time-to-fill metrics: [AVERAGES]\n",
        "- Employee engagement scores: [PROVIDE]\n",
        "- Compensation benchmarking: [POSITION VS MARKET]\n",
        "- Talent pipeline status: [DESCRIBE]\n",
        "- Diversity metrics: [PROVIDE]\n",
        "Analyze:\n",
        "1. Workforce composition vs. strategic needs\n",
        "2. Talent acquisition effectiveness\n",
        "3. Retention risks and drivers\n",
        "4. Engagement and productivity levels\n",
        "5. Diversity, equity, and inclusion status\n",
        "6. Workforce planning adequacy\n",
        "OUTPUT:\n",
        "- Workforce health scorecard\n",
        "- Critical talent risks and mitigation plans\n",
        "- Recruitment strategy optimization\n",
        "- Retention program enhancements\n",
        "- Workforce planning recommendations\n",
        "- HR metrics dashboard design\n",
        "\"\"\",\n",
        "\"PROMPT_9_ALIGNMENT\": \"\"\"Evaluate how well all seven elements of the 7S Framework align.\n",
        "INPUTS NEEDED:\n",
        "- Brief assessment of each S element: [PROVIDE STATUS]\n",
        "- Recent organizational changes: [LIST]\n",
        "- Performance metrics trends: [LAST 2 YEARS]\n",
        "- Strategic priorities: [TOP 5]\n",
        "- Known pain points: [DESCRIBE]\n",
        "- Success stories: [EXAMPLES]\n",
        "Assess:\n",
        "1. Element interdependencies and conflicts\n",
        "2. Alignment scoring for each element pair (21 combinations)\n",
        "3. Reinforcing vs. conflicting relationships\n",
        "4. Impact of misalignments on performance\n",
        "5. Root cause analysis of gaps\n",
        "6. Synergy opportunities\n",
        "OUTPUT:\n",
        "- 7S alignment matrix with scores\n",
        "- Critical misalignment identification\n",
        "- Dependency map visualization\n",
        "- Prioritized realignment initiatives\n",
        "- Change sequencing recommendations\n",
        "- Alignment monitoring framework\n",
        "\"\"\",\n",
        "\"PROMPT_10_CHANGE\": \"\"\"Use the 7S Framework to analyze organizational readiness for change.\n",
        "INPUTS NEEDED:\n",
        "- Planned change initiative: [DESCRIBE]\n",
        "- Change timeline and scope: [PROVIDE]\n",
        "- Previous change efforts: [SUCCESS/FAILURE EXAMPLES]\n",
        "- Stakeholder groups affected: [LIST]\n",
        "- Current change capability maturity: [ASSESS 1-5]\n",
        "- Resource availability: [BUDGET/PEOPLE]\n",
        "- Risk tolerance: [LOW/MEDIUM/HIGH]\n",
        "Analyze each S element for:\n",
        "1. Current state change readiness\n",
        "2. Required changes per element\n",
        "3. Resistance points and drivers\n",
        "4. Change capability gaps\n",
        "5. Success enablers and barriers\n",
        "6. Change impact assessment\n",
        "OUTPUT:\n",
        "- Change readiness score by S element\n",
        "- Resistance heat map\n",
        "- Change impact assessment matrix\n",
        "- Stakeholder engagement strategy\n",
        "- Change roadmap with quick wins\n",
        "- Risk mitigation plan\n",
        "- Success metrics framework\n",
        "\"\"\",\n",
        "\"PROMPT_11_DIGITAL\": \"\"\"Apply the 7S Framework to assess digital transformation readiness and impact.\n",
        "INPUTS NEEDED:\n",
        "- Current digital maturity: [ASSESS 1-5]\n",
        "- Digital strategy/initiatives: [DESCRIBE]\n",
        "- Technology infrastructure: [CURRENT STATE]\n",
        "- Digital skills inventory: [ASSESS]\n",
        "- Data and analytics capabilities: [DESCRIBE]\n",
        "- Customer digital expectations: [SUMMARIZE]\n",
        "- Competitor digital positioning: [BENCHMARK]\n",
        "Evaluate digital impact on:\n",
        "1. Strategy (digital business models)\n",
        "2. Structure (agile organization needs)\n",
        "3. Systems (technology architecture)\n",
        "4. Shared Values (digital culture)\n",
        "5. Style (digital leadership)\n",
        "6. Staff (digital talent)\n",
        "7. Skills (digital capabilities)\n",
        "OUTPUT:\n",
        "- Digital maturity assessment by S element\n",
        "- Digital transformation gaps and priorities\n",
        "- Technology investment recommendations\n",
        "- Digital culture transformation plan\n",
        "- Reskilling/upskilling requirements\n",
        "- Digital KPIs and governance model\n",
        "- Transformation roadmap with milestones\n",
        "\"\"\",\n",
        "\"PROMPT_12_BENCH\": \"\"\"Compare our 7S profile against key competitors.\n",
        "INPUTS NEEDED:\n",
        "- Top 3-5 competitors: [LIST]\n",
        "- Competitive intelligence available: [SUMMARIZE]\n",
        "- Industry best practices: [DESCRIBE]\n",
        "- Our performance vs. competitors: [METRICS]\n",
        "- Competitive advantages/disadvantages: [LIST]\n",
        "- Market position: [DESCRIBE]\n",
        "Benchmark:\n",
        "1. Strategy effectiveness comparison\n",
        "2. Organizational agility assessment\n",
        "3. Operational excellence comparison\n",
        "4. Cultural differentiation analysis\n",
        "5. Leadership capability comparison\n",
        "6. Talent competitiveness evaluation\n",
        "7. Innovation capability assessment\n",
        "OUTPUT:\n",
        "- Competitive 7S comparison matrix\n",
        "- Competitive advantage/disadvantage analysis\n",
        "- Best practice identification\n",
        "- Competitive gaps requiring closure\n",
        "- Differentiation opportunities\n",
        "- Competitive response strategies\n",
        "- Monitoring and intelligence framework\n",
        "\"\"\",\n",
        "\"PROMPT_13_GAPS\": \"\"\"Identify gaps between current and desired future state across the 7S elements.\n",
        "INPUTS NEEDED:\n",
        "- Vision for future state (3-5 years): [DESCRIBE]\n",
        "- Current state assessment: [SUMMARIZE BY S]\n",
        "- Strategic objectives: [LIST]\n",
        "- Performance targets: [SPECIFY]\n",
        "- Market/industry trends: [IDENTIFY]\n",
        "- Stakeholder expectations: [DESCRIBE]\n",
        "Analyze:\n",
        "1. Current state baseline for each S\n",
        "2. Future state requirements per S\n",
        "3. Gap magnitude and complexity\n",
        "4. Interdependency impact analysis\n",
        "5. Resource requirements for gap closure\n",
        "6. Timeline and sequencing needs\n",
        "OUTPUT:\n",
        "- Current vs. future state comparison table\n",
        "- Gap severity assessment (critical/high/medium/low)\n",
        "- Gap closure difficulty matrix\n",
        "- Investment requirements estimate\n",
        "- Transformation roadmap with phases\n",
        "- Quick wins vs. long-term initiatives\n",
        "- Success metrics and milestones\n",
        "\"\"\",\n",
        "\"PROMPT_14_INTEGRATION\": \"\"\"Combine 7S insights with SWOT or PESTLE analysis.\n",
        "INPUTS NEEDED:\n",
        "- 7S assessment summary: [PROVIDE]\n",
        "- SWOT analysis: [IF AVAILABLE]\n",
        "- PESTLE factors: [IF AVAILABLE]\n",
        "- Strategic options under consideration: [LIST]\n",
        "- Risk factors identified: [LIST]\n",
        "- Opportunity areas: [DESCRIBE]\n",
        "Integrate analyses to:\n",
        "1. Map external factors to internal capabilities\n",
        "2. Identify strategic option feasibility\n",
        "3. Assess implementation capabilities\n",
        "4. Determine competitive positioning\n",
        "5. Evaluate risk mitigation capacity\n",
        "6. Prioritize strategic initiatives\n",
        "OUTPUT:\n",
        "- Integrated strategy framework\n",
        "- Strategic option evaluation matrix\n",
        "- Capability-opportunity alignment map\n",
        "- Risk-readiness assessment\n",
        "- Strategic initiative prioritization\n",
        "- Implementation feasibility scores\n",
        "- Integrated dashboard design\n",
        "\"\"\",\n",
        "\"PROMPT_15_EXEC_SUMMARY\": \"\"\"Create an executive-level summary of 7S analysis with actionable recommendations.\n",
        "INPUTS NEEDED:\n",
        "- Full 7S analysis results: [PROVIDE KEY FINDINGS]\n",
        "- Strategic context and urgency: [DESCRIBE]\n",
        "- Available resources: [BUDGET/CAPACITY]\n",
        "- Board/Executive priorities: [LIST]\n",
        "- Key stakeholder concerns: [IDENTIFY]\n",
        "- Success criteria: [DEFINE]\n",
        "Synthesize:\n",
        "1. Critical insights from 7S analysis\n",
        "2. Top 3-5 strategic imperatives\n",
        "3. Quick wins vs. transformational changes\n",
        "4. Investment requirements and ROI\n",
        "5. Risk assessment and mitigation\n",
        "6. Implementation timeline\n",
        "OUTPUT:\n",
        "- 2-page executive summary\n",
        "- Visual 7S alignment dashboard\n",
        "- Top 10 recommendations ranked by impact\n",
        "- Investment and resource requirements\n",
        "- 90-day, 6-month, and 1-year action plans\n",
        "- Success metrics and governance model\n",
        "- Key risks and mitigation strategies\n",
        "- Next steps and decision requirements\n",
        "\"\"\",\n",
        "\"MEGA_PROMPT\": \"\"\"Act as a senior WRC consultant conducting a comprehensive 7S analysis. Provide a complete organizational assessment with actionable insights.\n",
        "[Use all context provided above and infer reasonable assumptions where N/A.]\n",
        "DELIVERABLES:\n",
        "- Executive Summary; Detailed Analysis by S; Integrated Findings; Strategic Recs; Implementation Plan; Appendices.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# Map bracketed placeholders to ORG fields\n",
        "PLACEHOLDER_MAP = {\n",
        "    \"[NAME]\": ORG.get(\"name\",\"\"),\n",
        "    \"[INDUSTRY]\": ORG.get(\"industry\",\"\"),\n",
        "    \"[NUMBER]\": ORG.get(\"size_employees\",\"\"),\n",
        "    \"[REVENUE]\": ORG.get(\"annual_revenue\",\"\"),\n",
        "    \"[LOCATIONS]\": ORG.get(\"locations\",\"\"),\n",
        "    \"[BRIEF DESCRIPTION]\": ORG.get(\"current_context\",\"\"),\n",
        "    \"[LIST 3-5]\": ORG.get(\"key_challenges\",\"\"),\n",
        "    \"[DESCRIBE]\": ORG.get(\"recent_changes\",\"\"),\n",
        "    \"[PROVIDE]\": ORG.get(\"strategy_statement\",\"\"),\n",
        "    \"[LIST]\": ORG.get(\"strategic_goals\",\"\"),\n",
        "    \"[STATEMENT]\": ORG.get(\"value_proposition\",\"\"),\n",
        "    \"[DESCRIBE HIERARCHY]\": ORG.get(\"org_chart\",\"\"),\n",
        "    \"[NUMBERS]\": ORG.get(\"span_of_control\",\"\"),\n",
        "    \"[IF AVAILABLE]\": \"\",\n",
        "    \"[AVERAGES]\": \"\",\n",
        "    \"[BREAKDOWN]\": \"\",\n",
        "    \"[DATA]\": \"\",\n",
        "    \"[POSITION VS MARKET]\": \"\",\n",
        "    \"[ASSESS 1-5]\": \"\",\n",
        "    \"[CURRENT STATE]\": \"\",\n",
        "    \"[ASSESS]\": \"\",\n",
        "    \"[SUMMARIZE]\": \"\",\n",
        "    \"[ANALYZE]\": \"\",\n",
        "    \"[IDENTIFY]\": \"\",\n",
        "    \"[SPECIFY]\": \"\"\n",
        "}\n",
        "\n",
        "def materialize_prompt(key: str) -> str:\n",
        "    text = PROMPTS[key]\n",
        "    for k,v in PLACEHOLDER_MAP.items():\n",
        "        text = text.replace(k, str(v if v else f\"N/A: {k}\"))\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "OsMRUxpNkI7v"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# üß† OLLAMA CHAT HELPERS ‚Äî Auto-detects generate/chat endpoint\n",
        "# ===============================================================\n",
        "import requests, json, time\n",
        "from typing import Dict\n",
        "\n",
        "TEMPERATURE, TOP_P, REPEAT_PENALTY, TIMEOUT_S = 0.4, 0.9, 1.1, 860\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Detect endpoint availability and set USE_GENERATE\n",
        "# ---------------------------------------------------------------\n",
        "BASE = \"http://127.0.0.1:11434\"\n",
        "GEN  = f\"{BASE}/api/generate\"\n",
        "CHAT = f\"{BASE}/api/chat\"\n",
        "\n",
        "def detect_generate_support():\n",
        "    try:\n",
        "        r = requests.options(GEN, timeout=3)\n",
        "        if r.status_code in (200, 204):\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "USE_GENERATE = detect_generate_support()\n",
        "print(f\"‚öôÔ∏è Using {'/api/generate' if USE_GENERATE else '/api/chat'} endpoint for inference.\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Ask LLM ‚Äî general wrapper\n",
        "# ---------------------------------------------------------------\n",
        "def ask_llm(model: str, sys: str, user: str) -> str:\n",
        "    \"\"\"Send message to Ollama and return text output.\"\"\"\n",
        "    try:\n",
        "        if USE_GENERATE:\n",
        "            # Simpler, faster API (no chat format)\n",
        "            prompt = f\"<<SYS>>\\n{sys}\\n<</SYS>>\\n\\n{user}\"\n",
        "            r = requests.post(\n",
        "                GEN,\n",
        "                json={\n",
        "                    \"model\": model,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False,\n",
        "                    \"options\": {\n",
        "                        \"temperature\": TEMPERATURE,\n",
        "                        \"top_p\": TOP_P,\n",
        "                        \"repeat_penalty\": REPEAT_PENALTY\n",
        "                    },\n",
        "                },\n",
        "                timeout=TIMEOUT_S,\n",
        "            )\n",
        "            r.raise_for_status()\n",
        "            return (r.json().get(\"response\") or \"\").strip()\n",
        "        else:\n",
        "            # Chat-style API\n",
        "            r = requests.post(\n",
        "                CHAT,\n",
        "                json={\n",
        "                    \"model\": model,\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"system\", \"content\": sys},\n",
        "                        {\"role\": \"user\", \"content\": user},\n",
        "                    ],\n",
        "                    \"stream\": False,\n",
        "                    \"options\": {\n",
        "                        \"temperature\": TEMPERATURE,\n",
        "                        \"top_p\": TOP_P,\n",
        "                        \"repeat_penalty\": REPEAT_PENALTY,\n",
        "                    },\n",
        "                },\n",
        "                timeout=TIMEOUT_S,\n",
        "            )\n",
        "            r.raise_for_status()\n",
        "            return ((r.json().get(\"message\") or {}).get(\"content\") or \"\").strip()\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        return \"‚ö†Ô∏è Request timed out ‚Äî model took too long.\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error querying model {model}: {e}\"\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Multi-model consensus synthesis\n",
        "# ---------------------------------------------------------------\n",
        "def synthesize_consensus(primary_model: str, prompt_name: str, per_model: Dict[str, str]) -> str:\n",
        "    \"\"\"Combine responses from multiple models into one cohesive expert synthesis.\"\"\"\n",
        "    sys = (\n",
        "        \"You are a senior WRC consultant. \"\n",
        "        \"Synthesize multiple expert drafts into one coherent, non-redundant, executive-ready report.\"\n",
        "    )\n",
        "    user = f\"\"\"Prompt: {prompt_name}\n",
        "\n",
        "Combine the following {len(per_model)} model answers into one structured response.\n",
        "Do NOT mention that this is a synthesis. Present it like a polished consultant report.\n",
        "\n",
        "Guidelines:\n",
        "- Keep the best points from each.\n",
        "- Merge ideas logically.\n",
        "- If answers conflict, show trade-offs and state your recommendation.\n",
        "- Use concise sections and bulleted clarity.\n",
        "\n",
        "=== MODEL ANSWERS ===\n",
        "{json.dumps(per_model, indent=2, ensure_ascii=False)}\n",
        "\"\"\"\n",
        "    return ask_llm(primary_model, sys, user)\n"
      ],
      "metadata": {
        "id": "MTKKxYIBkLgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddd51c9-578a-45b8-9242-d61e630e37a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Using /api/chat endpoint for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 5) RUN SELECTED PROMPTS ACROSS MODELS\n",
        "# ---------------------------\n",
        "def run_prompt_across_models(prompt_key: str, models: List[str]) -> Dict[str,str]:\n",
        "    sys = \"You are precise and to-the-point.\"\n",
        "    usr = materialize_prompt(prompt_key)\n",
        "    out = {}\n",
        "    for m in models:\n",
        "        try:\n",
        "            txt = ask_llm(m, sys, usr)\n",
        "            out[m] = txt\n",
        "            print(f\"  ‚úì {prompt_key} via {m}: {len(txt)} chars\")\n",
        "        except Exception as e:\n",
        "            out[m] = f\"(error from {m}: {e})\"\n",
        "            print(f\"  ‚úó {prompt_key} via {m}: {e}\")\n",
        "    return out\n",
        "\n",
        "results = {}\n",
        "consensus = {}\n",
        "print(\"\\nüß† Running prompts:\", \", \".join(RUN_PROMPTS))\n",
        "for key in RUN_PROMPTS:\n",
        "    per_model = run_prompt_across_models(key, REQUESTED_MODELS)\n",
        "    results[key] = per_model\n",
        "    consensus[key] = synthesize_consensus(PRIMARY_MODEL, key, per_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I2GPkTylQUJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d372a1f9-e63c-4d13-fe2b-5a64c4a15956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß† Running prompts: PROMPT_1_FULL_7S, PROMPT_2_STRATEGY, PROMPT_3_STRUCTURE, PROMPT_4_SYSTEMS, PROMPT_5_SHARED_VALUES, PROMPT_6_SKILLS, PROMPT_7_STYLE, PROMPT_8_STAFF, PROMPT_9_ALIGNMENT, PROMPT_10_CHANGE, PROMPT_11_DIGITAL, PROMPT_12_BENCH, PROMPT_13_GAPS, PROMPT_14_INTEGRATION, PROMPT_15_EXEC_SUMMARY, MEGA_PROMPT\n",
            "  ‚úì PROMPT_1_FULL_7S via mistral: 4282 chars\n",
            "  ‚úì PROMPT_1_FULL_7S via llama3: 4601 chars\n",
            "  ‚úì PROMPT_1_FULL_7S via gemma2: 5447 chars\n",
            "  ‚úì PROMPT_2_STRATEGY via mistral: 3168 chars\n",
            "  ‚úì PROMPT_2_STRATEGY via llama3: 3464 chars\n",
            "  ‚úì PROMPT_2_STRATEGY via gemma2: 4283 chars\n",
            "  ‚úì PROMPT_3_STRUCTURE via mistral: 3825 chars\n",
            "  ‚úì PROMPT_3_STRUCTURE via llama3: 4015 chars\n",
            "  ‚úì PROMPT_3_STRUCTURE via gemma2: 5498 chars\n",
            "  ‚úì PROMPT_4_SYSTEMS via mistral: 4052 chars\n",
            "  ‚úì PROMPT_4_SYSTEMS via llama3: 3581 chars\n",
            "  ‚úì PROMPT_4_SYSTEMS via gemma2: 6593 chars\n",
            "  ‚úì PROMPT_5_SHARED_VALUES via mistral: 3797 chars\n",
            "  ‚úì PROMPT_5_SHARED_VALUES via llama3: 4096 chars\n",
            "  ‚úì PROMPT_5_SHARED_VALUES via gemma2: 1020 chars\n",
            "  ‚úì PROMPT_6_SKILLS via mistral: 2921 chars\n",
            "  ‚úì PROMPT_6_SKILLS via llama3: 3121 chars\n",
            "  ‚úì PROMPT_6_SKILLS via gemma2: 3157 chars\n",
            "  ‚úì PROMPT_7_STYLE via mistral: 3412 chars\n",
            "  ‚úì PROMPT_7_STYLE via llama3: 3876 chars\n",
            "  ‚úì PROMPT_7_STYLE via gemma2: 3995 chars\n",
            "  ‚úì PROMPT_8_STAFF via mistral: 2663 chars\n",
            "  ‚úì PROMPT_8_STAFF via llama3: 3026 chars\n",
            "  ‚úì PROMPT_8_STAFF via gemma2: 4637 chars\n",
            "  ‚úì PROMPT_9_ALIGNMENT via mistral: 4118 chars\n",
            "  ‚úì PROMPT_9_ALIGNMENT via llama3: 3325 chars\n",
            "  ‚úì PROMPT_9_ALIGNMENT via gemma2: 2597 chars\n",
            "  ‚úì PROMPT_10_CHANGE via mistral: 7664 chars\n",
            "  ‚úì PROMPT_10_CHANGE via llama3: 7685 chars\n",
            "  ‚úì PROMPT_10_CHANGE via gemma2: 10136 chars\n",
            "  ‚úì PROMPT_11_DIGITAL via mistral: 4041 chars\n",
            "  ‚úì PROMPT_11_DIGITAL via llama3: 4972 chars\n",
            "  ‚úì PROMPT_11_DIGITAL via gemma2: 4808 chars\n",
            "  ‚úì PROMPT_12_BENCH via mistral: 2741 chars\n",
            "  ‚úì PROMPT_12_BENCH via llama3: 4158 chars\n",
            "  ‚úì PROMPT_12_BENCH via gemma2: 528 chars\n",
            "  ‚úì PROMPT_13_GAPS via mistral: 4118 chars\n",
            "  ‚úì PROMPT_13_GAPS via llama3: 6489 chars\n",
            "  ‚úì PROMPT_13_GAPS via gemma2: 5117 chars\n",
            "  ‚úì PROMPT_14_INTEGRATION via mistral: 2642 chars\n",
            "  ‚úì PROMPT_14_INTEGRATION via llama3: 2584 chars\n",
            "  ‚úì PROMPT_14_INTEGRATION via gemma2: 4188 chars\n",
            "  ‚úì PROMPT_15_EXEC_SUMMARY via mistral: 3887 chars\n",
            "  ‚úì PROMPT_15_EXEC_SUMMARY via llama3: 4366 chars\n",
            "  ‚úì PROMPT_15_EXEC_SUMMARY via gemma2: 1598 chars\n",
            "  ‚úì MEGA_PROMPT via mistral: 4088 chars\n",
            "  ‚úì MEGA_PROMPT via llama3: 3909 chars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# 6) SAVE OUTPUTS\n",
        "# ---------------------------\n",
        "bundle = {\n",
        "    \"metadata\": {\n",
        "        \"generated_at_utc\": datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
        "        \"ollama_host\": BASE,\n",
        "        \"models\": available,\n",
        "        \"primary_model\": PRIMARY_MODEL\n",
        "    },\n",
        "    \"org_inputs\": ORG,\n",
        "    \"raw_per_prompt\": results,\n",
        "    \"consensus_per_prompt\": consensus\n",
        "}\n",
        "\n",
        "with open(BUNDLE_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(bundle, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "with open(REPORT_MD, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"# 7S Report ‚Äî Consensus (models: {', '.join(available)})\\n\\n\")\n",
        "    f.write(\"## Organization\\n\")\n",
        "    f.write(\"```json\\n\" + json.dumps(ORG, indent=2, ensure_ascii=False) + \"\\n```\\n\\n\")\n",
        "    for key in RUN_PROMPTS:\n",
        "        f.write(f\"## {key}\\n\\n\")\n",
        "        f.write(consensus.get(key,\"\").strip() + \"\\n\\n\")\n",
        "        f.write(\"<details><summary>Model answers</summary>\\n\\n\")\n",
        "        f.write(\"```json\\n\" + json.dumps(results.get(key,{}), indent=2, ensure_ascii=False) + \"\\n```\\n\")\n",
        "        f.write(\"</details>\\n\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Done.\")\n",
        "print(\"JSON:\", BUNDLE_JSON)\n",
        "print(\"MD  :\", REPORT_MD)\n",
        "# =========================================================================================="
      ],
      "metadata": {
        "id": "VGhMDIGilhLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= DISPLAY 7S REPORT IN-CELL =======================\n",
        "import os, json, textwrap\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "OUT_DIR = \"/content\"\n",
        "BUNDLE_JSON = os.path.join(OUT_DIR, \"7S_bundle.json\")\n",
        "REPORT_MD   = os.path.join(OUT_DIR, \"7S_report.md\")\n",
        "\n",
        "def _h2(s):\n",
        "    return f\"\\n## {s}\\n\"\n",
        "def _h3(s):\n",
        "    return f\"\\n### {s}\\n\"\n",
        "def _codeblock(label, obj):\n",
        "    if isinstance(obj, str):\n",
        "        body = obj\n",
        "    else:\n",
        "        body = json.dumps(obj, indent=2, ensure_ascii=False)\n",
        "    return f\"\\n**{label}**\\n\\n```json\\n{body}\\n```\\n\"\n",
        "\n",
        "# 1) If the Markdown report exists, render that directly (richest output)\n",
        "if os.path.exists(REPORT_MD):\n",
        "    with open(REPORT_MD, \"r\", encoding=\"utf-8\") as f:\n",
        "        md = f.read()\n",
        "    display(Markdown(md))\n",
        "else:\n",
        "    # 2) Otherwise, pretty-print from the JSON bundle\n",
        "    if not os.path.exists(BUNDLE_JSON):\n",
        "        raise FileNotFoundError(\"No report found. Run the main cell first to create /content/7S_bundle.json or /content/7S_report.md.\")\n",
        "\n",
        "    with open(BUNDLE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        bundle = json.load(f)\n",
        "\n",
        "    meta     = bundle.get(\"metadata\", {})\n",
        "    org      = bundle.get(\"org_inputs\", {})\n",
        "    consensus = bundle.get(\"consensus_per_prompt\", {})\n",
        "    raw       = bundle.get(\"raw_per_prompt\", {})\n",
        "\n",
        "    # Build a friendly Markdown view\n",
        "    parts = []\n",
        "    title = f\"# 7S Report ‚Äî Consensus (models: {', '.join(meta.get('models', []))})\"\n",
        "    parts.append(title)\n",
        "    parts.append(_h2(\"Organization\"))\n",
        "    parts.append(\"```json\\n\" + json.dumps(org, indent=2, ensure_ascii=False) + \"\\n```\")\n",
        "\n",
        "    # Show consensus outputs section-by-section\n",
        "    for k in consensus.keys():\n",
        "        parts.append(_h2(k))\n",
        "        txt = consensus.get(k, \"\").strip()\n",
        "        if not txt:\n",
        "            parts.append(\"_No consensus text produced for this prompt._\")\n",
        "        else:\n",
        "            # Light formatting: ensure lines not too long for the notebook width\n",
        "            wrapped = \"\\n\".join(textwrap.fill(line, width=100) for line in txt.splitlines())\n",
        "            parts.append(wrapped)\n",
        "\n",
        "        # Collapsible raw model answers\n",
        "        parts.append(\"\\n<details><summary>Model answers</summary>\\n\\n\")\n",
        "        parts.append(\"```json\\n\" + json.dumps(raw.get(k, {}), indent=2, ensure_ascii=False) + \"\\n```\")\n",
        "        parts.append(\"\\n</details>\\n\")\n",
        "\n",
        "    display(Markdown(\"\\n\".join(parts)))\n",
        "# ========================================================================\n"
      ],
      "metadata": {
        "id": "SbVDZhmPGOgO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}